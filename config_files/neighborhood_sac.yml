env:
  BipedalWalker-v3
algo:
  sac
util:
  ["NeighborhoodNet", "OracleNeighborhoodNet", "InverseDynamicModule", "Discriminator"]
oracle_neighbor:
  False
wrapper_type:
  normobs
bc_only:
  False
no_bc:
  False
ood:
  False
hidden_dim:
  256
gamma:
  0.99
actor_optim:
  adam
critic_optim:
  adam
buffer_type:
  normal
buffer_to_tensor:
  True
td_length:
  1
soft_update_target:
  True
actor_lr:
  0.0003
critic_lr:
  0.0003
alpha_lr:
  0.0003
tau:
  0.0001
neighborhood_tau:
  0.0001
neighbor_model_alpha:
  0.99
episodes:
  200
seed:
  1
save_every:
  10
batch_size:
  256
buffer_size:
  1000000
main_stage_type:
  neighborhood_il
buffer_warmup:
  True
continue_training:
  False
buffer_warmup_step:
  2000
save_weight_period:
  50
use_ounoise:
  False
log_alpha_init:
  !!float 0
render:
  False
update_neighbor_frequency:
  3
update_neighbor_step:
  400
update_neighbor_until:
  500
discretize_reward:
  False
log_name:
  ""
duplicate_expert_last_state:
  False
data_name:
  ""
policy_threshold_ratio:
  10
auto_threshold_ratio:
  False
threshold_discount_factor:
  0.995
fix_env_random_seed:
  False
no_hard_negative_sampling:
  False
terminate_when_unhealthy:
  False
use_env_done:
  False
save_state_idx:
  True
use_target_neighbor:
  True
entropy_loss_weight_decay_rate:
  1
no_update_alpha:
  False
infinite_neighbor_buffer:
  False
bc_pretraining:
  False
hybrid:
  False
use_relative_reward:
  False
state_only:
  False
critic_without_entropy:
  False
target_entropy_weight:
  1
reward_scaling_weight:
  1.0
use_true_expert_relative_reward:
  False
low_hard_negative_weight:
  False
use_top_k:
  False
k_of_topk:
  1
use_pretrained_neighbor:
  False
pretrained_neighbor_weight_path:
  ""
expert_sub_sample_ratio:
  -1
use_IDM:
  False
max_episode_steps:
  -1
reset_as_expert_state:
  False
initial_state_key_to_add_noise:
  ["qpos","qvel"]
initial_state_noise_std:
  0.01
complementary_reward:
  False
only_use_relative_state:
  False
toy_reward_type:
  "gail"
use_discriminator:
  False
beta:
  0.1
total_timesteps:
  2000